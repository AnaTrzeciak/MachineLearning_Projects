{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2122282,"sourceType":"datasetVersion","datasetId":1273505}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-20T15:19:39.212715Z","iopub.execute_input":"2024-07-20T15:19:39.213187Z","iopub.status.idle":"2024-07-20T15:19:39.226369Z","shell.execute_reply.started":"2024-07-20T15:19:39.213151Z","shell.execute_reply":"2024-07-20T15:19:39.224890Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"/kaggle/input/superconductivty-data-data-set/unique_m.csv\n/kaggle/input/superconductivty-data-data-set/train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"There are two files: (1) train.csv contains 81 features extracted from 21263 superconductors along with the critical temperature in the 82nd column, (2) unique_m.csv contains the chemical formula broken up for all the 21263 superconductors from the train.csv file. The last two columns have the critical temperature and chemical formula. The goal here is to predict the critical temperature based on the features extracted, therefore, it is a univariate regression problem.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:39.235984Z","iopub.execute_input":"2024-07-20T15:19:39.237783Z","iopub.status.idle":"2024-07-20T15:19:39.244606Z","shell.execute_reply.started":"2024-07-20T15:19:39.237739Z","shell.execute_reply":"2024-07-20T15:19:39.243156Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"#Get the data\ndata_train = pd.read_csv('/kaggle/input/superconductivty-data-data-set/train.csv')\ndata_train.columns","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:39.261319Z","iopub.execute_input":"2024-07-20T15:19:39.262602Z","iopub.status.idle":"2024-07-20T15:19:39.729328Z","shell.execute_reply.started":"2024-07-20T15:19:39.262565Z","shell.execute_reply":"2024-07-20T15:19:39.727888Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"Index(['number_of_elements', 'mean_atomic_mass', 'wtd_mean_atomic_mass',\n       'gmean_atomic_mass', 'wtd_gmean_atomic_mass', 'entropy_atomic_mass',\n       'wtd_entropy_atomic_mass', 'range_atomic_mass', 'wtd_range_atomic_mass',\n       'std_atomic_mass', 'wtd_std_atomic_mass', 'mean_fie', 'wtd_mean_fie',\n       'gmean_fie', 'wtd_gmean_fie', 'entropy_fie', 'wtd_entropy_fie',\n       'range_fie', 'wtd_range_fie', 'std_fie', 'wtd_std_fie',\n       'mean_atomic_radius', 'wtd_mean_atomic_radius', 'gmean_atomic_radius',\n       'wtd_gmean_atomic_radius', 'entropy_atomic_radius',\n       'wtd_entropy_atomic_radius', 'range_atomic_radius',\n       'wtd_range_atomic_radius', 'std_atomic_radius', 'wtd_std_atomic_radius',\n       'mean_Density', 'wtd_mean_Density', 'gmean_Density',\n       'wtd_gmean_Density', 'entropy_Density', 'wtd_entropy_Density',\n       'range_Density', 'wtd_range_Density', 'std_Density', 'wtd_std_Density',\n       'mean_ElectronAffinity', 'wtd_mean_ElectronAffinity',\n       'gmean_ElectronAffinity', 'wtd_gmean_ElectronAffinity',\n       'entropy_ElectronAffinity', 'wtd_entropy_ElectronAffinity',\n       'range_ElectronAffinity', 'wtd_range_ElectronAffinity',\n       'std_ElectronAffinity', 'wtd_std_ElectronAffinity', 'mean_FusionHeat',\n       'wtd_mean_FusionHeat', 'gmean_FusionHeat', 'wtd_gmean_FusionHeat',\n       'entropy_FusionHeat', 'wtd_entropy_FusionHeat', 'range_FusionHeat',\n       'wtd_range_FusionHeat', 'std_FusionHeat', 'wtd_std_FusionHeat',\n       'mean_ThermalConductivity', 'wtd_mean_ThermalConductivity',\n       'gmean_ThermalConductivity', 'wtd_gmean_ThermalConductivity',\n       'entropy_ThermalConductivity', 'wtd_entropy_ThermalConductivity',\n       'range_ThermalConductivity', 'wtd_range_ThermalConductivity',\n       'std_ThermalConductivity', 'wtd_std_ThermalConductivity',\n       'mean_Valence', 'wtd_mean_Valence', 'gmean_Valence',\n       'wtd_gmean_Valence', 'entropy_Valence', 'wtd_entropy_Valence',\n       'range_Valence', 'wtd_range_Valence', 'std_Valence', 'wtd_std_Valence',\n       'critical_temp'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"data_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:39.731850Z","iopub.execute_input":"2024-07-20T15:19:39.732304Z","iopub.status.idle":"2024-07-20T15:19:39.770548Z","shell.execute_reply.started":"2024-07-20T15:19:39.732273Z","shell.execute_reply":"2024-07-20T15:19:39.769389Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n0                   4         88.944468             57.862692   \n1                   5         92.729214             58.518416   \n2                   4         88.944468             57.885242   \n3                   4         88.944468             57.873967   \n4                   4         88.944468             57.840143   \n5                   4         88.944468             57.795044   \n6                   4         88.944468             57.682296   \n7                   4         76.517718             57.175142   \n8                   4         76.517718             56.808817   \n9                   4         76.517718             56.442492   \n\n   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n0          66.361592              36.116612             1.181795   \n1          73.132787              36.396602             1.449309   \n2          66.361592              36.122509             1.181795   \n3          66.361592              36.119560             1.181795   \n4          66.361592              36.110716             1.181795   \n5          66.361592              36.098926             1.181795   \n6          66.361592              36.069470             1.181795   \n7          59.310096              35.891368             1.197273   \n8          59.310096              35.773432             1.197273   \n9          59.310096              35.655884             1.197273   \n\n   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n0                 1.062396          122.90607              31.794921   \n1                 1.057755          122.90607              36.161939   \n2                 0.975980          122.90607              35.741099   \n3                 1.022291          122.90607              33.768010   \n4                 1.129224          122.90607              27.848743   \n5                 1.225203          122.90607              20.687458   \n6                 1.316857          122.90607              10.765639   \n7                 0.943560          122.90607              36.451199   \n8                 0.981880          122.90607              34.833160   \n9                 1.016495          122.90607              33.215121   \n\n   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n0        51.968828  ...          2.257143       2.213364           2.219783   \n1        47.094633  ...          2.257143       1.888175           2.210679   \n2        51.968828  ...          2.271429       2.213364           2.232679   \n3        51.968828  ...          2.264286       2.213364           2.226222   \n4        51.968828  ...          2.242857       2.213364           2.206963   \n5        51.968828  ...          2.214286       2.213364           2.181543   \n6        51.968828  ...          2.142857       2.213364           2.119268   \n7        44.289459  ...          2.271429       2.213364           2.232679   \n8        44.289459  ...          2.264286       2.213364           2.226222   \n9        44.289459  ...          2.257143       2.213364           2.219783   \n\n   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n0         1.368922             1.066221              1           1.085714   \n1         1.557113             1.047221              2           1.128571   \n2         1.368922             1.029175              1           1.114286   \n3         1.368922             1.048834              1           1.100000   \n4         1.368922             1.096052              1           1.057143   \n5         1.368922             1.141474              1           1.000000   \n6         1.368922             1.194453              1           0.857143   \n7         1.368922             1.029175              1           1.114286   \n8         1.368922             1.048834              1           1.100000   \n9         1.368922             1.066221              1           1.085714   \n\n   std_Valence  wtd_std_Valence  critical_temp  \n0     0.433013         0.437059           29.0  \n1     0.632456         0.468606           26.0  \n2     0.433013         0.444697           19.0  \n3     0.433013         0.440952           22.0  \n4     0.433013         0.428809           23.0  \n5     0.433013         0.410326           23.0  \n6     0.433013         0.349927           11.0  \n7     0.433013         0.444697           33.0  \n8     0.433013         0.440952           36.0  \n9     0.433013         0.437059           31.0  \n\n[10 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number_of_elements</th>\n      <th>mean_atomic_mass</th>\n      <th>wtd_mean_atomic_mass</th>\n      <th>gmean_atomic_mass</th>\n      <th>wtd_gmean_atomic_mass</th>\n      <th>entropy_atomic_mass</th>\n      <th>wtd_entropy_atomic_mass</th>\n      <th>range_atomic_mass</th>\n      <th>wtd_range_atomic_mass</th>\n      <th>std_atomic_mass</th>\n      <th>...</th>\n      <th>wtd_mean_Valence</th>\n      <th>gmean_Valence</th>\n      <th>wtd_gmean_Valence</th>\n      <th>entropy_Valence</th>\n      <th>wtd_entropy_Valence</th>\n      <th>range_Valence</th>\n      <th>wtd_range_Valence</th>\n      <th>std_Valence</th>\n      <th>wtd_std_Valence</th>\n      <th>critical_temp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.862692</td>\n      <td>66.361592</td>\n      <td>36.116612</td>\n      <td>1.181795</td>\n      <td>1.062396</td>\n      <td>122.90607</td>\n      <td>31.794921</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>2.213364</td>\n      <td>2.219783</td>\n      <td>1.368922</td>\n      <td>1.066221</td>\n      <td>1</td>\n      <td>1.085714</td>\n      <td>0.433013</td>\n      <td>0.437059</td>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>92.729214</td>\n      <td>58.518416</td>\n      <td>73.132787</td>\n      <td>36.396602</td>\n      <td>1.449309</td>\n      <td>1.057755</td>\n      <td>122.90607</td>\n      <td>36.161939</td>\n      <td>47.094633</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>1.888175</td>\n      <td>2.210679</td>\n      <td>1.557113</td>\n      <td>1.047221</td>\n      <td>2</td>\n      <td>1.128571</td>\n      <td>0.632456</td>\n      <td>0.468606</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.885242</td>\n      <td>66.361592</td>\n      <td>36.122509</td>\n      <td>1.181795</td>\n      <td>0.975980</td>\n      <td>122.90607</td>\n      <td>35.741099</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.271429</td>\n      <td>2.213364</td>\n      <td>2.232679</td>\n      <td>1.368922</td>\n      <td>1.029175</td>\n      <td>1</td>\n      <td>1.114286</td>\n      <td>0.433013</td>\n      <td>0.444697</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.873967</td>\n      <td>66.361592</td>\n      <td>36.119560</td>\n      <td>1.181795</td>\n      <td>1.022291</td>\n      <td>122.90607</td>\n      <td>33.768010</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.264286</td>\n      <td>2.213364</td>\n      <td>2.226222</td>\n      <td>1.368922</td>\n      <td>1.048834</td>\n      <td>1</td>\n      <td>1.100000</td>\n      <td>0.433013</td>\n      <td>0.440952</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.840143</td>\n      <td>66.361592</td>\n      <td>36.110716</td>\n      <td>1.181795</td>\n      <td>1.129224</td>\n      <td>122.90607</td>\n      <td>27.848743</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.242857</td>\n      <td>2.213364</td>\n      <td>2.206963</td>\n      <td>1.368922</td>\n      <td>1.096052</td>\n      <td>1</td>\n      <td>1.057143</td>\n      <td>0.433013</td>\n      <td>0.428809</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.795044</td>\n      <td>66.361592</td>\n      <td>36.098926</td>\n      <td>1.181795</td>\n      <td>1.225203</td>\n      <td>122.90607</td>\n      <td>20.687458</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.214286</td>\n      <td>2.213364</td>\n      <td>2.181543</td>\n      <td>1.368922</td>\n      <td>1.141474</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>0.433013</td>\n      <td>0.410326</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.682296</td>\n      <td>66.361592</td>\n      <td>36.069470</td>\n      <td>1.181795</td>\n      <td>1.316857</td>\n      <td>122.90607</td>\n      <td>10.765639</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.142857</td>\n      <td>2.213364</td>\n      <td>2.119268</td>\n      <td>1.368922</td>\n      <td>1.194453</td>\n      <td>1</td>\n      <td>0.857143</td>\n      <td>0.433013</td>\n      <td>0.349927</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4</td>\n      <td>76.517718</td>\n      <td>57.175142</td>\n      <td>59.310096</td>\n      <td>35.891368</td>\n      <td>1.197273</td>\n      <td>0.943560</td>\n      <td>122.90607</td>\n      <td>36.451199</td>\n      <td>44.289459</td>\n      <td>...</td>\n      <td>2.271429</td>\n      <td>2.213364</td>\n      <td>2.232679</td>\n      <td>1.368922</td>\n      <td>1.029175</td>\n      <td>1</td>\n      <td>1.114286</td>\n      <td>0.433013</td>\n      <td>0.444697</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4</td>\n      <td>76.517718</td>\n      <td>56.808817</td>\n      <td>59.310096</td>\n      <td>35.773432</td>\n      <td>1.197273</td>\n      <td>0.981880</td>\n      <td>122.90607</td>\n      <td>34.833160</td>\n      <td>44.289459</td>\n      <td>...</td>\n      <td>2.264286</td>\n      <td>2.213364</td>\n      <td>2.226222</td>\n      <td>1.368922</td>\n      <td>1.048834</td>\n      <td>1</td>\n      <td>1.100000</td>\n      <td>0.433013</td>\n      <td>0.440952</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4</td>\n      <td>76.517718</td>\n      <td>56.442492</td>\n      <td>59.310096</td>\n      <td>35.655884</td>\n      <td>1.197273</td>\n      <td>1.016495</td>\n      <td>122.90607</td>\n      <td>33.215121</td>\n      <td>44.289459</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>2.213364</td>\n      <td>2.219783</td>\n      <td>1.368922</td>\n      <td>1.066221</td>\n      <td>1</td>\n      <td>1.085714</td>\n      <td>0.433013</td>\n      <td>0.437059</td>\n      <td>31.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 82 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:39.772005Z","iopub.execute_input":"2024-07-20T15:19:39.772511Z","iopub.status.idle":"2024-07-20T15:19:39.800458Z","shell.execute_reply.started":"2024-07-20T15:19:39.772479Z","shell.execute_reply":"2024-07-20T15:19:39.798888Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 21263 entries, 0 to 21262\nData columns (total 82 columns):\n #   Column                           Non-Null Count  Dtype  \n---  ------                           --------------  -----  \n 0   number_of_elements               21263 non-null  int64  \n 1   mean_atomic_mass                 21263 non-null  float64\n 2   wtd_mean_atomic_mass             21263 non-null  float64\n 3   gmean_atomic_mass                21263 non-null  float64\n 4   wtd_gmean_atomic_mass            21263 non-null  float64\n 5   entropy_atomic_mass              21263 non-null  float64\n 6   wtd_entropy_atomic_mass          21263 non-null  float64\n 7   range_atomic_mass                21263 non-null  float64\n 8   wtd_range_atomic_mass            21263 non-null  float64\n 9   std_atomic_mass                  21263 non-null  float64\n 10  wtd_std_atomic_mass              21263 non-null  float64\n 11  mean_fie                         21263 non-null  float64\n 12  wtd_mean_fie                     21263 non-null  float64\n 13  gmean_fie                        21263 non-null  float64\n 14  wtd_gmean_fie                    21263 non-null  float64\n 15  entropy_fie                      21263 non-null  float64\n 16  wtd_entropy_fie                  21263 non-null  float64\n 17  range_fie                        21263 non-null  float64\n 18  wtd_range_fie                    21263 non-null  float64\n 19  std_fie                          21263 non-null  float64\n 20  wtd_std_fie                      21263 non-null  float64\n 21  mean_atomic_radius               21263 non-null  float64\n 22  wtd_mean_atomic_radius           21263 non-null  float64\n 23  gmean_atomic_radius              21263 non-null  float64\n 24  wtd_gmean_atomic_radius          21263 non-null  float64\n 25  entropy_atomic_radius            21263 non-null  float64\n 26  wtd_entropy_atomic_radius        21263 non-null  float64\n 27  range_atomic_radius              21263 non-null  int64  \n 28  wtd_range_atomic_radius          21263 non-null  float64\n 29  std_atomic_radius                21263 non-null  float64\n 30  wtd_std_atomic_radius            21263 non-null  float64\n 31  mean_Density                     21263 non-null  float64\n 32  wtd_mean_Density                 21263 non-null  float64\n 33  gmean_Density                    21263 non-null  float64\n 34  wtd_gmean_Density                21263 non-null  float64\n 35  entropy_Density                  21263 non-null  float64\n 36  wtd_entropy_Density              21263 non-null  float64\n 37  range_Density                    21263 non-null  float64\n 38  wtd_range_Density                21263 non-null  float64\n 39  std_Density                      21263 non-null  float64\n 40  wtd_std_Density                  21263 non-null  float64\n 41  mean_ElectronAffinity            21263 non-null  float64\n 42  wtd_mean_ElectronAffinity        21263 non-null  float64\n 43  gmean_ElectronAffinity           21263 non-null  float64\n 44  wtd_gmean_ElectronAffinity       21263 non-null  float64\n 45  entropy_ElectronAffinity         21263 non-null  float64\n 46  wtd_entropy_ElectronAffinity     21263 non-null  float64\n 47  range_ElectronAffinity           21263 non-null  float64\n 48  wtd_range_ElectronAffinity       21263 non-null  float64\n 49  std_ElectronAffinity             21263 non-null  float64\n 50  wtd_std_ElectronAffinity         21263 non-null  float64\n 51  mean_FusionHeat                  21263 non-null  float64\n 52  wtd_mean_FusionHeat              21263 non-null  float64\n 53  gmean_FusionHeat                 21263 non-null  float64\n 54  wtd_gmean_FusionHeat             21263 non-null  float64\n 55  entropy_FusionHeat               21263 non-null  float64\n 56  wtd_entropy_FusionHeat           21263 non-null  float64\n 57  range_FusionHeat                 21263 non-null  float64\n 58  wtd_range_FusionHeat             21263 non-null  float64\n 59  std_FusionHeat                   21263 non-null  float64\n 60  wtd_std_FusionHeat               21263 non-null  float64\n 61  mean_ThermalConductivity         21263 non-null  float64\n 62  wtd_mean_ThermalConductivity     21263 non-null  float64\n 63  gmean_ThermalConductivity        21263 non-null  float64\n 64  wtd_gmean_ThermalConductivity    21263 non-null  float64\n 65  entropy_ThermalConductivity      21263 non-null  float64\n 66  wtd_entropy_ThermalConductivity  21263 non-null  float64\n 67  range_ThermalConductivity        21263 non-null  float64\n 68  wtd_range_ThermalConductivity    21263 non-null  float64\n 69  std_ThermalConductivity          21263 non-null  float64\n 70  wtd_std_ThermalConductivity      21263 non-null  float64\n 71  mean_Valence                     21263 non-null  float64\n 72  wtd_mean_Valence                 21263 non-null  float64\n 73  gmean_Valence                    21263 non-null  float64\n 74  wtd_gmean_Valence                21263 non-null  float64\n 75  entropy_Valence                  21263 non-null  float64\n 76  wtd_entropy_Valence              21263 non-null  float64\n 77  range_Valence                    21263 non-null  int64  \n 78  wtd_range_Valence                21263 non-null  float64\n 79  std_Valence                      21263 non-null  float64\n 80  wtd_std_Valence                  21263 non-null  float64\n 81  critical_temp                    21263 non-null  float64\ndtypes: float64(79), int64(3)\nmemory usage: 13.3 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"data_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:39.803799Z","iopub.execute_input":"2024-07-20T15:19:39.804312Z","iopub.status.idle":"2024-07-20T15:19:40.067704Z","shell.execute_reply.started":"2024-07-20T15:19:39.804278Z","shell.execute_reply":"2024-07-20T15:19:40.066575Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\ncount        21263.000000      21263.000000          21263.000000   \nmean             4.115224         87.557631             72.988310   \nstd              1.439295         29.676497             33.490406   \nmin              1.000000          6.941000              6.423452   \n25%              3.000000         72.458076             52.143839   \n50%              4.000000         84.922750             60.696571   \n75%              5.000000        100.404410             86.103540   \nmax              9.000000        208.980400            208.980400   \n\n       gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\ncount       21263.000000           21263.000000         21263.000000   \nmean           71.290627              58.539916             1.165608   \nstd            31.030272              36.651067             0.364930   \nmin             5.320573               1.960849             0.000000   \n25%            58.041225              35.248990             0.966676   \n50%            66.361592              39.918385             1.199541   \n75%            78.116681              73.113234             1.444537   \nmax           208.980400             208.980400             1.983797   \n\n       wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\ncount             21263.000000       21263.000000           21263.000000   \nmean                  1.063884         115.601251              33.225218   \nstd                   0.401423          54.626887              26.967752   \nmin                   0.000000           0.000000               0.000000   \n25%                   0.775363          78.512902              16.824174   \n50%                   1.146783         122.906070              26.636008   \n75%                   1.359418         154.119320              38.356908   \nmax                   1.958203         207.972460             205.589910   \n\n       std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  \\\ncount     21263.000000  ...      21263.000000   21263.000000   \nmean         44.391893  ...          3.153127       3.056536   \nstd          20.035430  ...          1.191249       1.046257   \nmin           0.000000  ...          1.000000       1.000000   \n25%          32.890369  ...          2.116732       2.279705   \n50%          45.123500  ...          2.618182       2.615321   \n75%          59.322812  ...          4.026201       3.727919   \nmax         101.019700  ...          7.000000       7.000000   \n\n       wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\ncount       21263.000000     21263.000000         21263.000000   21263.000000   \nmean            3.055885         1.295682             1.052841       2.041010   \nstd             1.174815         0.393155             0.380291       1.242345   \nmin             1.000000         0.000000             0.000000       0.000000   \n25%             2.091251         1.060857             0.775678       1.000000   \n50%             2.434057         1.368922             1.166532       2.000000   \n75%             3.914868         1.589027             1.330801       3.000000   \nmax             7.000000         2.141963             1.949739       6.000000   \n\n       wtd_range_Valence   std_Valence  wtd_std_Valence  critical_temp  \ncount       21263.000000  21263.000000     21263.000000   21263.000000  \nmean            1.483007      0.839342         0.673987      34.421219  \nstd             0.978176      0.484676         0.455580      34.254362  \nmin             0.000000      0.000000         0.000000       0.000210  \n25%             0.921454      0.451754         0.306892       5.365000  \n50%             1.063077      0.800000         0.500000      20.000000  \n75%             1.918400      1.200000         1.020436      63.000000  \nmax             6.992200      3.000000         3.000000     185.000000  \n\n[8 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number_of_elements</th>\n      <th>mean_atomic_mass</th>\n      <th>wtd_mean_atomic_mass</th>\n      <th>gmean_atomic_mass</th>\n      <th>wtd_gmean_atomic_mass</th>\n      <th>entropy_atomic_mass</th>\n      <th>wtd_entropy_atomic_mass</th>\n      <th>range_atomic_mass</th>\n      <th>wtd_range_atomic_mass</th>\n      <th>std_atomic_mass</th>\n      <th>...</th>\n      <th>wtd_mean_Valence</th>\n      <th>gmean_Valence</th>\n      <th>wtd_gmean_Valence</th>\n      <th>entropy_Valence</th>\n      <th>wtd_entropy_Valence</th>\n      <th>range_Valence</th>\n      <th>wtd_range_Valence</th>\n      <th>std_Valence</th>\n      <th>wtd_std_Valence</th>\n      <th>critical_temp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>...</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n      <td>21263.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.115224</td>\n      <td>87.557631</td>\n      <td>72.988310</td>\n      <td>71.290627</td>\n      <td>58.539916</td>\n      <td>1.165608</td>\n      <td>1.063884</td>\n      <td>115.601251</td>\n      <td>33.225218</td>\n      <td>44.391893</td>\n      <td>...</td>\n      <td>3.153127</td>\n      <td>3.056536</td>\n      <td>3.055885</td>\n      <td>1.295682</td>\n      <td>1.052841</td>\n      <td>2.041010</td>\n      <td>1.483007</td>\n      <td>0.839342</td>\n      <td>0.673987</td>\n      <td>34.421219</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.439295</td>\n      <td>29.676497</td>\n      <td>33.490406</td>\n      <td>31.030272</td>\n      <td>36.651067</td>\n      <td>0.364930</td>\n      <td>0.401423</td>\n      <td>54.626887</td>\n      <td>26.967752</td>\n      <td>20.035430</td>\n      <td>...</td>\n      <td>1.191249</td>\n      <td>1.046257</td>\n      <td>1.174815</td>\n      <td>0.393155</td>\n      <td>0.380291</td>\n      <td>1.242345</td>\n      <td>0.978176</td>\n      <td>0.484676</td>\n      <td>0.455580</td>\n      <td>34.254362</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>6.941000</td>\n      <td>6.423452</td>\n      <td>5.320573</td>\n      <td>1.960849</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000210</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.000000</td>\n      <td>72.458076</td>\n      <td>52.143839</td>\n      <td>58.041225</td>\n      <td>35.248990</td>\n      <td>0.966676</td>\n      <td>0.775363</td>\n      <td>78.512902</td>\n      <td>16.824174</td>\n      <td>32.890369</td>\n      <td>...</td>\n      <td>2.116732</td>\n      <td>2.279705</td>\n      <td>2.091251</td>\n      <td>1.060857</td>\n      <td>0.775678</td>\n      <td>1.000000</td>\n      <td>0.921454</td>\n      <td>0.451754</td>\n      <td>0.306892</td>\n      <td>5.365000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.000000</td>\n      <td>84.922750</td>\n      <td>60.696571</td>\n      <td>66.361592</td>\n      <td>39.918385</td>\n      <td>1.199541</td>\n      <td>1.146783</td>\n      <td>122.906070</td>\n      <td>26.636008</td>\n      <td>45.123500</td>\n      <td>...</td>\n      <td>2.618182</td>\n      <td>2.615321</td>\n      <td>2.434057</td>\n      <td>1.368922</td>\n      <td>1.166532</td>\n      <td>2.000000</td>\n      <td>1.063077</td>\n      <td>0.800000</td>\n      <td>0.500000</td>\n      <td>20.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>5.000000</td>\n      <td>100.404410</td>\n      <td>86.103540</td>\n      <td>78.116681</td>\n      <td>73.113234</td>\n      <td>1.444537</td>\n      <td>1.359418</td>\n      <td>154.119320</td>\n      <td>38.356908</td>\n      <td>59.322812</td>\n      <td>...</td>\n      <td>4.026201</td>\n      <td>3.727919</td>\n      <td>3.914868</td>\n      <td>1.589027</td>\n      <td>1.330801</td>\n      <td>3.000000</td>\n      <td>1.918400</td>\n      <td>1.200000</td>\n      <td>1.020436</td>\n      <td>63.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.000000</td>\n      <td>208.980400</td>\n      <td>208.980400</td>\n      <td>208.980400</td>\n      <td>208.980400</td>\n      <td>1.983797</td>\n      <td>1.958203</td>\n      <td>207.972460</td>\n      <td>205.589910</td>\n      <td>101.019700</td>\n      <td>...</td>\n      <td>7.000000</td>\n      <td>7.000000</td>\n      <td>7.000000</td>\n      <td>2.141963</td>\n      <td>1.949739</td>\n      <td>6.000000</td>\n      <td>6.992200</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>185.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 82 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_unique = pd.read_csv('/kaggle/input/superconductivty-data-data-set/unique_m.csv')\ndata_unique.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.068922Z","iopub.execute_input":"2024-07-20T15:19:40.069297Z","iopub.status.idle":"2024-07-20T15:19:40.288428Z","shell.execute_reply.started":"2024-07-20T15:19:40.069259Z","shell.execute_reply":"2024-07-20T15:19:40.286975Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"     H  He   Li   Be    B    C    N    O    F  Ne  ...   Au   Hg   Tl   Pb  \\\n0  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n1  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n2  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n3  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n4  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n5  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n6  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n7  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n8  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n9  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n\n    Bi  Po  At  Rn  critical_temp                material  \n0  0.0   0   0   0           29.0         Ba0.2La1.8Cu1O4  \n1  0.0   0   0   0           26.0  Ba0.1La1.9Ag0.1Cu0.9O4  \n2  0.0   0   0   0           19.0         Ba0.1La1.9Cu1O4  \n3  0.0   0   0   0           22.0       Ba0.15La1.85Cu1O4  \n4  0.0   0   0   0           23.0         Ba0.3La1.7Cu1O4  \n5  0.0   0   0   0           23.0         Ba0.5La1.5Cu1O4  \n6  0.0   0   0   0           11.0             Ba1La1Cu1O4  \n7  0.0   0   0   0           33.0         Sr0.1La1.9Cu1O4  \n8  0.0   0   0   0           36.0       Sr0.15La1.85Cu1O4  \n9  0.0   0   0   0           31.0         Sr0.2La1.8Cu1O4  \n\n[10 rows x 88 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>H</th>\n      <th>He</th>\n      <th>Li</th>\n      <th>Be</th>\n      <th>B</th>\n      <th>C</th>\n      <th>N</th>\n      <th>O</th>\n      <th>F</th>\n      <th>Ne</th>\n      <th>...</th>\n      <th>Au</th>\n      <th>Hg</th>\n      <th>Tl</th>\n      <th>Pb</th>\n      <th>Bi</th>\n      <th>Po</th>\n      <th>At</th>\n      <th>Rn</th>\n      <th>critical_temp</th>\n      <th>material</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>29.0</td>\n      <td>Ba0.2La1.8Cu1O4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>Ba0.1La1.9Ag0.1Cu0.9O4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19.0</td>\n      <td>Ba0.1La1.9Cu1O4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>Ba0.15La1.85Cu1O4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.0</td>\n      <td>Ba0.3La1.7Cu1O4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.0</td>\n      <td>Ba0.5La1.5Cu1O4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11.0</td>\n      <td>Ba1La1Cu1O4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33.0</td>\n      <td>Sr0.1La1.9Cu1O4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36.0</td>\n      <td>Sr0.15La1.85Cu1O4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31.0</td>\n      <td>Sr0.2La1.8Cu1O4</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 88 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#data_unique.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.290023Z","iopub.execute_input":"2024-07-20T15:19:40.290513Z","iopub.status.idle":"2024-07-20T15:19:40.296124Z","shell.execute_reply.started":"2024-07-20T15:19:40.290469Z","shell.execute_reply":"2024-07-20T15:19:40.294823Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Lets try to visualize the data and get some information. A quick way to get a feel of the type of data you are dealing with is to plot a histogram or correlation map for each numerical attribute. However, we have a lot of features (82), so then, it is difficult to understand anything. Hence, we will plot the correlation map only with the target variable.","metadata":{}},{"cell_type":"code","source":"#Correlation between feature and critical tempeature\ncorrelations = data_train.corr()[\"critical_temp\"].sort_values(ascending=False).drop(\"critical_temp\")\nfor feature, correlation in correlations.items():\n    print(f\"{feature} : {correlation:.6f}\")\n\n#fig, axes = plt.subplots(nrows=9, ncols=9, figsize=(20, 20))\n#axes = axes.flatten()\n#for i, feature in enumerate(correlations.index):\n#    sns.scatterplot(x=data_train[feature], y=data_train[\"critical_temp\"], ax=axes[i])\n#    axes[i].set_title(f'Corr: {correlations[feature]:.2f}')\n#    axes[i].set_xlabel(feature)\n#    axes[i].set_ylabel(\"critical_temp\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.297793Z","iopub.execute_input":"2024-07-20T15:19:40.298237Z","iopub.status.idle":"2024-07-20T15:19:40.710708Z","shell.execute_reply.started":"2024-07-20T15:19:40.298187Z","shell.execute_reply":"2024-07-20T15:19:40.709502Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"wtd_std_ThermalConductivity : 0.721271\nrange_ThermalConductivity : 0.687654\nrange_atomic_radius : 0.653759\nstd_ThermalConductivity : 0.653632\nwtd_entropy_atomic_mass : 0.626930\nwtd_entropy_atomic_radius : 0.603494\nnumber_of_elements : 0.601069\nrange_fie : 0.600790\nwtd_std_atomic_radius : 0.599199\nentropy_Valence : 0.598591\nwtd_entropy_Valence : 0.589664\nwtd_std_fie : 0.582013\nentropy_fie : 0.567817\nwtd_entropy_FusionHeat : 0.563244\nstd_atomic_radius : 0.559629\nentropy_atomic_radius : 0.558937\nentropy_FusionHeat : 0.552709\nentropy_atomic_mass : 0.543619\nstd_fie : 0.541804\nrange_atomic_mass : 0.491970\nwtd_range_ThermalConductivity : 0.469572\nentropy_Density : 0.457169\nentropy_ElectronAffinity : 0.437207\nwtd_entropy_Density : 0.400190\nwtd_mean_fie : 0.398796\nwtd_entropy_fie : 0.388359\nwtd_mean_ThermalConductivity : 0.379336\nstd_atomic_mass : 0.378766\nmean_ThermalConductivity : 0.375813\nwtd_std_atomic_mass : 0.359306\nwtd_gmean_fie : 0.343747\nwtd_std_ElectronAffinity : 0.315147\nwtd_range_fie : 0.300482\nrange_ElectronAffinity : 0.279705\nstd_ElectronAffinity : 0.262103\nrange_Density : 0.260536\nwtd_entropy_ElectronAffinity : 0.237648\nwtd_std_Density : 0.207663\nwtd_range_ElectronAffinity : 0.185348\nstd_Density : 0.115243\nwtd_mean_ElectronAffinity : 0.111516\nmean_atomic_radius : 0.105273\nmean_fie : 0.102268\nentropy_ThermalConductivity : 0.085862\ngmean_fie : -0.025103\nwtd_gmean_ElectronAffinity : -0.107359\nmean_atomic_mass : -0.113523\nwtd_entropy_ThermalConductivity : -0.116728\nrange_FusionHeat : -0.140714\nrange_Valence : -0.143546\ngmean_atomic_radius : -0.143770\nmean_ElectronAffinity : -0.193550\nwtd_std_FusionHeat : -0.195571\nstd_FusionHeat : -0.201310\nstd_Valence : -0.208072\ngmean_atomic_mass : -0.230345\nwtd_range_Density : -0.284729\nwtd_mean_atomic_radius : -0.297272\nwtd_std_Valence : -0.300028\nwtd_mean_atomic_mass : -0.312272\nwtd_range_FusionHeat : -0.314178\nwtd_range_atomic_mass : -0.337131\nwtd_range_atomic_radius : -0.344100\nmean_Density : -0.368262\nwtd_gmean_atomic_mass : -0.369858\nwtd_gmean_ThermalConductivity : -0.371601\ngmean_ElectronAffinity : -0.380568\nmean_FusionHeat : -0.385509\ngmean_ThermalConductivity : -0.387192\nwtd_mean_FusionHeat : -0.394117\nwtd_gmean_atomic_radius : -0.405176\ngmean_FusionHeat : -0.431795\nwtd_gmean_FusionHeat : -0.432365\nwtd_mean_Density : -0.433940\nwtd_range_Valence : -0.439901\nwtd_gmean_Density : -0.540046\ngmean_Density : -0.541684\ngmean_Valence : -0.573068\nmean_Valence : -0.600085\nwtd_gmean_Valence : -0.615653\nwtd_mean_Valence : -0.632401\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation (directly proportional) while the negative sign indicates that the relationship between the variables is inversely proportional. Finally, coefficients close to zero mean that there is no linear correlation. Therefore, at first we cannot discard any attribute. In addiction, if we take a look at the scatterplot, none of the attributes have a completely linear relationship with the target. ","metadata":{}},{"cell_type":"code","source":"#data_train.plot(kind=\"scatter\", x=\"mean_Valence\", y=\"critical_temp\", alpha=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.712363Z","iopub.execute_input":"2024-07-20T15:19:40.712836Z","iopub.status.idle":"2024-07-20T15:19:40.718370Z","shell.execute_reply.started":"2024-07-20T15:19:40.712789Z","shell.execute_reply":"2024-07-20T15:19:40.716918Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"y = data_train[\"critical_temp\"]\nX = data_train.drop('critical_temp', axis= 1)\nprint(y.shape)\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.719822Z","iopub.execute_input":"2024-07-20T15:19:40.720221Z","iopub.status.idle":"2024-07-20T15:19:40.738249Z","shell.execute_reply.started":"2024-07-20T15:19:40.720170Z","shell.execute_reply":"2024-07-20T15:19:40.736873Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"(21263,)\n(21263, 81)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales, so then, one of the most important transformations we need to apply  is feature scaling. Note that \"entropy_atomic_mass\" has 0.36 as minimun value and 1.98 as a maximun value while for \"gmean_atomic_mass\" the range is 5 to > 200. \n\n- Lets try MinMaxScaler: where values are shifted and rescaled so that they end up ranging from 0 to 1.","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX_norm = scaler.fit_transform(X)\nprint(X_norm)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.743265Z","iopub.execute_input":"2024-07-20T15:19:40.743672Z","iopub.status.idle":"2024-07-20T15:19:40.780099Z","shell.execute_reply.started":"2024-07-20T15:19:40.743641Z","shell.execute_reply":"2024-07-20T15:19:40.778713Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"[[0.375      0.40587859 0.25394952 ... 0.15527506 0.14433757 0.14568627]\n [0.5        0.42461131 0.25718675 ... 0.16140434 0.21081851 0.15620209]\n [0.375      0.40587859 0.25406085 ... 0.15936125 0.14433757 0.14823221]\n ...\n [0.125      0.45893123 0.44029915 ... 0.45765281 0.16666667 0.13333333]\n [0.125      0.45893123 0.44763782 ... 0.31606647 0.16666667 0.15416441]\n [0.25       0.39857242 0.39709844 ... 0.25742971 0.47140452 0.5       ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.20, random_state = 2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.781594Z","iopub.execute_input":"2024-07-20T15:19:40.782178Z","iopub.status.idle":"2024-07-20T15:19:40.811582Z","shell.execute_reply.started":"2024-07-20T15:19:40.782126Z","shell.execute_reply":"2024-07-20T15:19:40.810547Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"(17010, 81)\n(4253, 81)\n(17010,)\n(4253,)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Select and train a model\nlinearModel = LinearRegression()\nlinearModel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.812858Z","iopub.execute_input":"2024-07-20T15:19:40.813213Z","iopub.status.idle":"2024-07-20T15:19:40.892151Z","shell.execute_reply.started":"2024-07-20T15:19:40.813167Z","shell.execute_reply":"2024-07-20T15:19:40.890605Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"LinearRegression()","text/html":"<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Just a reminder, Root Mean Square Error (RMSE) is given by\n\n\\begin{equation}\nRMSE = \\left[\\frac{1}{m}\\sum_{i = 1}^{m} (y_{pred}^{i}  - y_{true}^{i})^{2}\\right]^{1/2}\n\\end{equation}","metadata":{}},{"cell_type":"code","source":"#Let’s measure this regression model’s RMSE on the whole training set\ny_pred_linear = linearModel.predict(x_train)\nlinear_mse = mean_squared_error(y_train, y_pred_linear)\nlinear_rmse = np.sqrt(linear_mse)\nprint(linear_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.894290Z","iopub.execute_input":"2024-07-20T15:19:40.894900Z","iopub.status.idle":"2024-07-20T15:19:40.916492Z","shell.execute_reply.started":"2024-07-20T15:19:40.894850Z","shell.execute_reply":"2024-07-20T15:19:40.914844Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"17.62091784166398\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Okay, this is better than nothing but it not a great score. Critical temperature range is between 0.000210 and 185.0, so a predict error of 17 is not satisfying. Its look like a underfitting case. Lets try another model:","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nmodelDecTree = DecisionTreeRegressor()\nmodelDecTree.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:40.918632Z","iopub.execute_input":"2024-07-20T15:19:40.920421Z","iopub.status.idle":"2024-07-20T15:19:42.432314Z","shell.execute_reply.started":"2024-07-20T15:19:40.920367Z","shell.execute_reply":"2024-07-20T15:19:42.431095Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"DecisionTreeRegressor()","text/html":"<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred_tree = modelDecTree.predict(x_train)\ntree_mse = mean_squared_error(y_train, y_pred_tree)\ntree_rmse = np.sqrt(tree_mse)\nprint(tree_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:42.435226Z","iopub.execute_input":"2024-07-20T15:19:42.435713Z","iopub.status.idle":"2024-07-20T15:19:42.451216Z","shell.execute_reply.started":"2024-07-20T15:19:42.435669Z","shell.execute_reply":"2024-07-20T15:19:42.449867Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"4.169956434668739\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Despite DecisionTree sound better, lets try to evaluate in a different way. A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. cross_val_score is a crucial function in scikit-learn for evaluating model performance through cross-validation. It splits your data into multiple folds, trains your model on a subset of the data, and evaluates it on the remaining fold. This process is repeated for each fold, providing a more robust estimate of model performance.\n\n- The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n\n- **scoring: The metric to evaluate the model's performance. Default is 'accuracy' for classification and 'r2' for regression.** \n\n- $R^{2}$ score: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(modelDecTree, x_train, y_train, cv=10)\ntree_rmse_scores = np.sqrt(scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:42.453046Z","iopub.execute_input":"2024-07-20T15:19:42.453432Z","iopub.status.idle":"2024-07-20T15:19:55.571919Z","shell.execute_reply.started":"2024-07-20T15:19:42.453401Z","shell.execute_reply":"2024-07-20T15:19:55.570708Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"print(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard deviation:\", scores.std())","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:55.573496Z","iopub.execute_input":"2024-07-20T15:19:55.573835Z","iopub.status.idle":"2024-07-20T15:19:55.581103Z","shell.execute_reply.started":"2024-07-20T15:19:55.573808Z","shell.execute_reply":"2024-07-20T15:19:55.579761Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Scores: [0.88504592 0.8782531  0.86086645 0.85920511 0.88302593 0.84812239\n 0.86947889 0.85630411 0.89740792 0.87056106]\nMean: 0.8708270880578652\nStandard deviation: 0.014405784412219784\n","output_type":"stream"}]},{"cell_type":"code","source":"scores_linear = cross_val_score(linearModel, x_train, y_train, cv=10)\nlinear_rmse_scores = np.sqrt(scores_linear)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:55.582662Z","iopub.execute_input":"2024-07-20T15:19:55.583126Z","iopub.status.idle":"2024-07-20T15:19:56.788429Z","shell.execute_reply.started":"2024-07-20T15:19:55.583087Z","shell.execute_reply":"2024-07-20T15:19:56.786544Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"print(\"Scores:\", scores_linear)\nprint(\"Mean:\", scores_linear.mean())\nprint(\"Standard deviation:\", scores_linear.std())","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:56.791597Z","iopub.execute_input":"2024-07-20T15:19:56.793935Z","iopub.status.idle":"2024-07-20T15:19:56.808403Z","shell.execute_reply.started":"2024-07-20T15:19:56.793874Z","shell.execute_reply":"2024-07-20T15:19:56.806235Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Scores: [0.73193664 0.74790308 0.70006707 0.74014222 0.74032142 0.74343039\n 0.7342585  0.70692134 0.73987232 0.74472307]\nMean: 0.7329576047797729\nStandard deviation: 0.015451665624078362\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Based on the results, both models look good, with the Decicion Tree model being slightly better.","metadata":{}},{"cell_type":"code","source":"#Evaluate the model on the test set\nfinal_pred_linear = linearModel.predict(x_test)\nfinal_pred_DecTree = modelDecTree.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:19:56.811882Z","iopub.execute_input":"2024-07-20T15:19:56.813990Z","iopub.status.idle":"2024-07-20T15:19:56.835422Z","shell.execute_reply.started":"2024-07-20T15:19:56.813921Z","shell.execute_reply":"2024-07-20T15:19:56.833633Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"final_linear_mse = mean_squared_error(y_test, final_pred_linear)\nfinal_linear_rmse = np.sqrt(final_linear_mse)\nprint(\"RMSE for linear model on test data= \", final_linear_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:22:42.875872Z","iopub.execute_input":"2024-07-20T15:22:42.876340Z","iopub.status.idle":"2024-07-20T15:22:42.884272Z","shell.execute_reply.started":"2024-07-20T15:22:42.876304Z","shell.execute_reply":"2024-07-20T15:22:42.882869Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"RMSE for linear model on test data=  17.344578083438407\n","output_type":"stream"}]},{"cell_type":"code","source":"final_DecTree_mse = mean_squared_error(y_test, final_pred_DecTree)\nfinal_DecTree_rmse = np.sqrt(final_DecTree_mse)\nprint(\"RMSE for Decision Tree model on test set= \", final_DecTree_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T15:23:21.751176Z","iopub.execute_input":"2024-07-20T15:23:21.751657Z","iopub.status.idle":"2024-07-20T15:23:21.760407Z","shell.execute_reply.started":"2024-07-20T15:23:21.751626Z","shell.execute_reply":"2024-07-20T15:23:21.758990Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"RMSE for Decision Tree model on test set=  12.316154109346865\n","output_type":"stream"}]},{"cell_type":"markdown","source":" - Here we see an overfittin problem in decision tree model. This model works well on the traning set, however, for new datasets it does not have a good prediction. We need to fine-tune it. \n - Linear model look like \"better\". \n \n - Possible ways to reduce this error:\n\n    - eliminate irrelevant attributes, but from the analysis of the data made at the beginning, apparently all of them seem relevant. We can do it using RandomForestRegressor to see the importance of each attribute for making accurate predictions\n \n- However, looking at the code of other people who analyzed the same data set, other models like GradientBoostingRegressor, Cat_BOOST and XGB, were used and the results were not that better. They were even used with attribute reduction.","metadata":{}}]}